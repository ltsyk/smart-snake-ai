#!/usr/bin/env python3
"""
è¯„ä¼°ç°æœ‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°
"""
import argparse
import os
import sys
import numpy as np
import torch
import torch.nn as nn
from statistics import mean, stdev
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))
from src.env import SnakeEnv

class DQNNet(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
    def forward(self, x):
        return self.net(x)

def evaluate_model(model_path, num_episodes=100, max_steps=1000, verbose=False):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    # åŠ è½½æ¨¡å‹
    model = DQNNet(400, 4)
    state_dict = torch.load(model_path, map_location='cpu')
    model.load_state_dict(state_dict)
    model.eval()
    
    env = SnakeEnv()
    scores = []
    episode_lengths = []
    
    print(f"å¼€å§‹è¯„ä¼°æ¨¡å‹ (å…± {num_episodes} è½®)...")
    
    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while steps < max_steps:
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
                q_values = model(state_tensor)
                action = torch.argmax(q_values, dim=1).item()
            
            next_state, reward, done, _ = env.step(action)
            total_reward += reward
            state = next_state
            steps += 1
            
            if done:
                break
        
        scores.append(env.game.score)
        episode_lengths.append(steps)
        
        if verbose and (episode + 1) % 20 == 0:
            print(f"Episode {episode + 1}/{num_episodes}: Score = {env.game.score}, Steps = {steps}")
    
    return scores, episode_lengths

def analyze_performance(scores, episode_lengths):
    """åˆ†ææ€§èƒ½æŒ‡æ ‡"""
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # å¾—åˆ†ç»Ÿè®¡
    print(f"ğŸ¯ å¾—åˆ†ç»Ÿè®¡:")
    print(f"   å¹³å‡å¾—åˆ†: {mean(scores):.2f}")
    print(f"   æœ€é«˜å¾—åˆ†: {max(scores)}")
    print(f"   æœ€ä½å¾—åˆ†: {min(scores)}")
    if len(scores) > 1:
        print(f"   æ ‡å‡†å·®: {stdev(scores):.2f}")
    
    # æ­¥æ•°ç»Ÿè®¡
    print(f"\nğŸƒ æ­¥æ•°ç»Ÿè®¡:")
    print(f"   å¹³å‡æ­¥æ•°: {mean(episode_lengths):.1f}")
    print(f"   æœ€é•¿æ¸¸æˆ: {max(episode_lengths)} æ­¥")
    print(f"   æœ€çŸ­æ¸¸æˆ: {min(episode_lengths)} æ­¥")
    
    # æˆåŠŸç‡åˆ†æ
    successful_games = len([s for s in scores if s > 0])
    success_rate = successful_games / len(scores) * 100
    print(f"\nğŸ® æ¸¸æˆè¡¨ç°:")
    print(f"   æˆåŠŸå¾—åˆ†æ¸¸æˆ: {successful_games}/{len(scores)} ({success_rate:.1f}%)")
    
    # å¾—åˆ†åˆ†å¸ƒ
    score_distribution = {}
    for score in scores:
        score_range = f"{score//5*5}-{score//5*5+4}" if score > 0 else "0"
        score_distribution[score_range] = score_distribution.get(score_range, 0) + 1
    
    print(f"\nğŸ“ˆ å¾—åˆ†åˆ†å¸ƒ:")
    for score_range in sorted(score_distribution.keys(), key=lambda x: int(x.split('-')[0])):
        count = score_distribution[score_range]
        percentage = count / len(scores) * 100
        print(f"   {score_range}: {count} games ({percentage:.1f}%)")
    
    return {
        'mean_score': mean(scores),
        'max_score': max(scores),
        'success_rate': success_rate,
        'mean_steps': mean(episode_lengths)
    }

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°æ¨¡å‹æ€§èƒ½')
    parser.add_argument('--model_path', type=str, default='models/dqn_snake.pt',
                        help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=100,
                        help='è¯„ä¼°è½®æ•°')
    parser.add_argument('--max_steps', type=int, default=1000,
                        help='æ¯è½®æœ€å¤§æ­¥æ•°')
    parser.add_argument('--verbose', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    args = parser.parse_args()
    
    scores, episode_lengths = evaluate_model(
        args.model_path, args.episodes, args.max_steps, args.verbose
    )
    
    metrics = analyze_performance(scores, episode_lengths)
    
    # ä¿å­˜ç»“æœ
    results_file = 'evaluation_results.txt'
    with open(results_file, 'w') as f:
        f.write(f"Model: {args.model_path}\n")
        f.write(f"Episodes: {args.episodes}\n")
        f.write(f"Mean Score: {metrics['mean_score']:.2f}\n")
        f.write(f"Max Score: {metrics['max_score']}\n")
        f.write(f"Success Rate: {metrics['success_rate']:.1f}%\n")
        f.write(f"Mean Steps: {metrics['mean_steps']:.1f}\n")
        f.write(f"All Scores: {scores}\n")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

if __name__ == '__main__':
    main()