#!/usr/bin/env python3
"""
è¯„ä¼°æ”¹è¿›ç‰ˆæ¨¡å‹çš„æ€§èƒ½è¡¨ç°
"""
import argparse
import os
import sys
import numpy as np
import torch
import torch.nn as nn
from statistics import mean, stdev
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__))))
from src.env_improved import ImprovedSnakeEnv

class ImprovedDQN(nn.Module):
    """ä¸è®­ç»ƒä¸€è‡´çš„Dueling DQNç»“æ„"""
    def __init__(self, input_dim, output_dim, hidden_size=256):
        super(ImprovedDQN, self).__init__()
        self.feature = nn.Sequential(
            nn.Linear(input_dim, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, 1)
        )
        self.adv_stream = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_dim)
        )

    def forward(self, x):
        features = self.feature(x)
        value = self.value_stream(features)
        adv = self.adv_stream(features)
        return value + adv - adv.mean(dim=1, keepdim=True)

def evaluate_improved_model(model_path, num_episodes=100, max_steps=1000, verbose=False):
    """è¯„ä¼°æ”¹è¿›æ¨¡å‹æ€§èƒ½"""
    # åŠ è½½æ¨¡å‹
    model = ImprovedDQN(408, 4, hidden_size=128)  # ä½¿ç”¨è®­ç»ƒæ—¶çš„å‚æ•°
    state_dict = torch.load(model_path, map_location='cpu')
    model.load_state_dict(state_dict)
    model.eval()
    
    env = ImprovedSnakeEnv()
    scores = []
    episode_lengths = []
    total_rewards = []
    
    print(f"å¼€å§‹è¯„ä¼°æ”¹è¿›æ¨¡å‹ (å…± {num_episodes} è½®)...")
    
    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while steps < max_steps:
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
                q_values = model(state_tensor)
                action = torch.argmax(q_values, dim=1).item()
            
            next_state, reward, done, info = env.step(action)
            total_reward += reward
            state = next_state
            steps += 1
            
            if done:
                break
        
        scores.append(info.get('score', 0))
        episode_lengths.append(steps)
        total_rewards.append(total_reward)
        
        if verbose and (episode + 1) % 20 == 0:
            print(f"Episode {episode + 1}/{num_episodes}: Score = {scores[-1]}, Steps = {steps}, Reward = {total_reward:.1f}")
    
    return scores, episode_lengths, total_rewards

def compare_models(original_results, improved_results):
    """å¯¹æ¯”åŸå§‹å’Œæ”¹è¿›æ¨¡å‹çš„æ€§èƒ½"""
    print("\n" + "="*70)
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("="*70)
    
    orig_scores, orig_lengths = original_results
    imp_scores, imp_lengths, imp_rewards = improved_results
    
    print(f"{'æŒ‡æ ‡':<20} | {'åŸå§‹æ¨¡å‹':<15} | {'æ”¹è¿›æ¨¡å‹':<15} | {'æå‡':<15}")
    print("-" * 70)
    
    # å¹³å‡å¾—åˆ†å¯¹æ¯”
    orig_mean = mean(orig_scores)
    imp_mean = mean(imp_scores)
    improvement = ((imp_mean - orig_mean) / max(orig_mean, 0.01)) * 100
    print(f"{'å¹³å‡å¾—åˆ†':<20} | {orig_mean:<15.2f} | {imp_mean:<15.2f} | {improvement:+.1f}%")
    
    # æœ€é«˜å¾—åˆ†å¯¹æ¯”
    orig_max = max(orig_scores)
    imp_max = max(imp_scores)
    print(f"{'æœ€é«˜å¾—åˆ†':<20} | {orig_max:<15d} | {imp_max:<15d} | {imp_max - orig_max:+d}")
    
    # æˆåŠŸç‡å¯¹æ¯”
    orig_success = len([s for s in orig_scores if s > 0]) / len(orig_scores) * 100
    imp_success = len([s for s in imp_scores if s > 0]) / len(imp_scores) * 100
    print(f"{'æˆåŠŸç‡':<20} | {orig_success:<15.1f}% | {imp_success:<15.1f}% | {imp_success - orig_success:+.1f}%")
    
    # å¹³å‡æ­¥æ•°å¯¹æ¯”
    orig_steps = mean(orig_lengths)
    imp_steps = mean(imp_lengths)
    print(f"{'å¹³å‡æ­¥æ•°':<20} | {orig_steps:<15.1f} | {imp_steps:<15.1f} | {imp_steps - orig_steps:+.1f}")
    
    print(f"{'å¹³å‡å¥–åŠ±':<20} | {'N/A':<15} | {mean(imp_rewards):<15.1f} | {'New Metric':<15}")

def analyze_improved_performance(scores, episode_lengths, total_rewards):
    """åˆ†ææ”¹è¿›æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡"""
    print("\n" + "="*60)
    print("ğŸ“Š æ”¹è¿›æ¨¡å‹è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # å¾—åˆ†ç»Ÿè®¡
    print(f"ğŸ¯ å¾—åˆ†ç»Ÿè®¡:")
    print(f"   å¹³å‡å¾—åˆ†: {mean(scores):.2f}")
    print(f"   æœ€é«˜å¾—åˆ†: {max(scores)}")
    print(f"   æœ€ä½å¾—åˆ†: {min(scores)}")
    if len(scores) > 1:
        print(f"   æ ‡å‡†å·®: {stdev(scores):.2f}")
    
    # å¥–åŠ±ç»Ÿè®¡
    print(f"\nğŸ’° å¥–åŠ±ç»Ÿè®¡:")
    print(f"   å¹³å‡å¥–åŠ±: {mean(total_rewards):.2f}")
    print(f"   æœ€é«˜å¥–åŠ±: {max(total_rewards):.1f}")
    print(f"   æœ€ä½å¥–åŠ±: {min(total_rewards):.1f}")
    
    # æ­¥æ•°ç»Ÿè®¡
    print(f"\nğŸƒ æ­¥æ•°ç»Ÿè®¡:")
    print(f"   å¹³å‡æ­¥æ•°: {mean(episode_lengths):.1f}")
    print(f"   æœ€é•¿æ¸¸æˆ: {max(episode_lengths)} æ­¥")
    print(f"   æœ€çŸ­æ¸¸æˆ: {min(episode_lengths)} æ­¥")
    
    # æˆåŠŸç‡åˆ†æ
    successful_games = len([s for s in scores if s > 0])
    success_rate = successful_games / len(scores) * 100
    print(f"\nğŸ® æ¸¸æˆè¡¨ç°:")
    print(f"   æˆåŠŸå¾—åˆ†æ¸¸æˆ: {successful_games}/{len(scores)} ({success_rate:.1f}%)")
    
    # é«˜åˆ†æ¸¸æˆåˆ†æ
    high_score_games = len([s for s in scores if s >= 2])
    high_score_rate = high_score_games / len(scores) * 100
    print(f"   é«˜åˆ†æ¸¸æˆ(â‰¥2åˆ†): {high_score_games}/{len(scores)} ({high_score_rate:.1f}%)")
    
    return {
        'mean_score': mean(scores),
        'max_score': max(scores),
        'success_rate': success_rate,
        'mean_steps': mean(episode_lengths),
        'mean_reward': mean(total_rewards)
    }

def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°æ”¹è¿›æ¨¡å‹æ€§èƒ½')
    parser.add_argument('--improved_model', type=str, default='models/improved_dqn_snake.pt',
                        help='æ”¹è¿›æ¨¡å‹è·¯å¾„')
    parser.add_argument('--original_model', type=str, default='models/dqn_snake.pt',
                        help='åŸå§‹æ¨¡å‹è·¯å¾„')
    parser.add_argument('--episodes', type=int, default=50,
                        help='è¯„ä¼°è½®æ•°')
    parser.add_argument('--max_steps', type=int, default=1000,
                        help='æ¯è½®æœ€å¤§æ­¥æ•°')
    parser.add_argument('--verbose', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--compare', action='store_true',
                        help='ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”')
    args = parser.parse_args()
    
    # è¯„ä¼°æ”¹è¿›æ¨¡å‹
    imp_scores, imp_lengths, imp_rewards = evaluate_improved_model(
        args.improved_model, args.episodes, args.max_steps, args.verbose
    )
    
    metrics = analyze_improved_performance(imp_scores, imp_lengths, imp_rewards)
    
    # å¦‚æœéœ€è¦å¯¹æ¯”
    if args.compare:
        try:
            # è¯»å–åŸå§‹æ¨¡å‹è¯„ä¼°ç»“æœ
            with open('evaluation_results.txt', 'r') as f:
                lines = f.readlines()
                orig_scores = eval(lines[-1].split(': ')[1])
            
            # ç®€å•è®¡ç®—åŸå§‹æ¨¡å‹æ­¥æ•°ï¼ˆå‡è®¾å¤§éƒ¨åˆ†è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼‰
            orig_lengths = [1000 if s == 0 else 100 for s in orig_scores]  # ç®€åŒ–ä¼°è®¡
            
            compare_models((orig_scores, orig_lengths), (imp_scores, imp_lengths, imp_rewards))
        except:
            print("\nâš ï¸  æ— æ³•è¯»å–åŸå§‹æ¨¡å‹è¯„ä¼°ç»“æœï¼Œè·³è¿‡å¯¹æ¯”")
    
    # ä¿å­˜ç»“æœ
    results_file = 'improved_evaluation_results.txt'
    with open(results_file, 'w') as f:
        f.write(f"Improved Model: {args.improved_model}\n")
        f.write(f"Episodes: {args.episodes}\n")
        f.write(f"Mean Score: {metrics['mean_score']:.2f}\n")
        f.write(f"Max Score: {metrics['max_score']}\n")
        f.write(f"Success Rate: {metrics['success_rate']:.1f}%\n")
        f.write(f"Mean Steps: {metrics['mean_steps']:.1f}\n")
        f.write(f"Mean Reward: {metrics['mean_reward']:.2f}\n")
        f.write(f"All Scores: {imp_scores}\n")
        f.write(f"All Rewards: {imp_rewards}\n")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

if __name__ == '__main__':
    main()
